---
title: "Likelihood and Model Quality"
---

## Motivation: Why do we need loss functions?

When training neural networks, we always minimize a loss function.
However, loss functions are often introduced as if they were arbitrary design choices.

In this note, we explain where loss functions come from by starting with a probabilistic view of model quality, based on likelihood.

This perspective will naturally lead us to negative log-likelihood and common loss functions such as least squares.

---

## What problem does likelihood solve?




Likelihood measures how well a model with **fixed parameters** explains the observed data.

Given a model and a dataset, likelihood answers the question:
> If the model parameters were correct, how plausible would the observed data be?

Importantly, likelihood is not a probability over parameters, but a function of the parameters given the data.

---

## Why is likelihood a product over data points?

In most machine learning settings, we assume that training data points are **independent and identically distributed (i.i.d.)**.

Under this assumption, the joint probability of the dataset factorizes into a product of per-sample probabilities.
As a result, the likelihood over the dataset becomes a product over individual data points.

This factorization is a modeling assumption rather than a mathematical necessity.

---

## From likelihood to log-likelihood

Maximizing the likelihood directly can be numerically unstable, since it involves multiplying many small probabilities.

To address this, we maximize the **log-likelihood** instead.
Because the logarithm is a monotonic function, maximizing the log-likelihood yields the same optimal parameters as maximizing the likelihood.

In addition, the log transform converts products into sums, which is more convenient for both analysis and optimization.

---

## From log-likelihood to loss minimization

Machine learning optimization is typically framed as a **minimization** problem rather than a maximization problem.

Therefore, instead of maximizing the log-likelihood, we minimize its negative.
This leads to the definition of the **negative log-likelihood (NLL)**.

By defining the loss function as the negative log-likelihood, training a model becomes equivalent to minimizing a loss function during optimization.

---

## Least squares as a special case

Under the assumption that prediction errors follow a Gaussian distribution with fixed variance, the negative log-likelihood reduces to the familiar least squares loss.

This shows that commonly used loss functions are not arbitrary, but arise naturally from probabilistic modeling assumptions.

(Details to be expanded.)

---
